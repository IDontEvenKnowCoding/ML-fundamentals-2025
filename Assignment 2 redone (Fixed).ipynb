{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/IDontEvenKnowCoding/ML-fundamentals-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import zscore\n",
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and normalize column names to prevent hidden duplicates\n",
    "data = pd.read_csv('hour.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.columns.str.strip().str.lower()  # Ensures consistent naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows and info\n",
    "print(\"Head of data:\")\n",
    "print(data.head())\n",
    "print(\"\\nData Info:\")\n",
    "print(data.info())\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot target variable distribution (cnt)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['cnt'], kde=True)\n",
    "plt.title('Distribution of Bike Rentals Count')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(data['cnt'].mean(), color='red', linestyle='--', label=f\"Mean: {data['cnt'].mean():.1f}\")\n",
    "plt.axvline(data['cnt'].median(), color='green', linestyle='--', label=f\"Median: {data['cnt'].median()}\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Skewness of target variable: {data['cnt'].skew()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional EDA plots (hour, weekday, month, season, weather, etc.)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='hr', y='cnt', data=data)\n",
    "plt.title('Bike Rentals by Hour')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='weekday', y='cnt', data=data)\n",
    "plt.title('Bike Rentals by Day of Week')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='mnth', y='cnt', data=data)\n",
    "plt.title('Bike Rentals by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='season', y='cnt', data=data)\n",
    "plt.title('Bike Rentals by Season')\n",
    "plt.xlabel('Season (1:Winter, 2:Spring, 3:Summer, 4:Fall)')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather-related scatter plots\n",
    "weather_features = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "for i, feature in enumerate(weather_features):\n",
    "    sns.scatterplot(x=feature, y='cnt', data=data, alpha=0.3, ax=axes[i])\n",
    "    axes[i].set_title(f'Bike Rentals vs {feature}')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='weathersit', y='cnt', data=data)\n",
    "plt.title('Bike Rentals by Weather Situation')\n",
    "plt.xlabel('Weather Situation (1:Clear, 2:Mist, 3:Light Snow/Rain)')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Convert dteday to datetime for correlation analysis\n",
    "data['dteday'] = pd.to_datetime(data['dteday'])\n",
    "\n",
    "# Correlation analysis\n",
    "plt.figure(figsize=(14, 10))\n",
    "correlation_matrix = data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check correlation between temp and atemp specifically\n",
    "print(f\"Correlation between temp and atemp: {correlation_matrix.loc['temp', 'atemp']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IQR-based bounds for 'temp'\n",
    "Q1 = data['temp'].quantile(0.25)\n",
    "Q3 = data['temp'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Plot the distribution of 'temp' with outlier bounds\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['temp'], bins=30, kde=True)\n",
    "plt.axvline(lower_bound, color='red', linestyle='--', label=f'Lower Bound: {lower_bound:.2f}')\n",
    "plt.axvline(upper_bound, color='green', linestyle='--', label=f'Upper Bound: {upper_bound:.2f}')\n",
    "plt.title('Temperature Distribution with IQR Outlier Bounds')\n",
    "plt.xlabel('Temperature')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate IQR for 'windspeed'\n",
    "Q1_windspeed = data['windspeed'].quantile(0.25)\n",
    "Q3_windspeed = data['windspeed'].quantile(0.75)\n",
    "IQR_windspeed = Q3_windspeed - Q1_windspeed\n",
    "lower_bound_windspeed = Q1_windspeed - 1.5 * IQR_windspeed\n",
    "upper_bound_windspeed = Q3_windspeed + 1.5 * IQR_windspeed\n",
    "# Plot the distribution of 'windspeed' with outlier bounds\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['windspeed'], bins=30, kde=True)\n",
    "plt.axvline(lower_bound_windspeed, color='red', linestyle='--', label=f'Lower Bound: {lower_bound_windspeed:.2f}')\n",
    "plt.axvline(upper_bound_windspeed, color='green', linestyle='--', label=f'Upper Bound: {upper_bound_windspeed:.2f}')\n",
    "plt.title('Windspeed Distribution with IQR Outlier Bounds')\n",
    "plt.xlabel('Windspeed')\n",
    "plt.legend()\n",
    "plt.show() \n",
    "\n",
    "#Calculate IQR for 'hum'\n",
    "Q1_hum = data['hum'].quantile(0.25)\n",
    "Q3_hum = data['hum'].quantile(0.75)\n",
    "IQR_hum = Q3_hum - Q1_hum\n",
    "lower_bound_hum = Q1_hum - 1.5 * IQR_hum\n",
    "upper_bound_hum = Q3_hum + 1.5 * IQR_hum\n",
    "# Plot the distribution of 'hum' with outlier bounds\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['hum'], bins=30, kde=True)\n",
    "plt.axvline(lower_bound_hum, color='red', linestyle='--', label=f'Lower Bound: {lower_bound_hum:.2f}')\n",
    "plt.axvline(upper_bound_hum, color='green', linestyle='--', label=f'Upper Bound: {upper_bound_hum:.2f}')\n",
    "plt.title('Humidity Distribution with IQR Outlier Bounds')\n",
    "plt.xlabel('Humidity')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Outlier detection using z-score\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Compute the z-score for 'cnt'\n",
    "data['cnt_zscore'] = zscore(data['cnt'])\n",
    "\n",
    "# Plot a scatter plot of bike rentals over time with outliers highlighted (z-score > 3)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=data.index, y='cnt', data=data, hue=data['cnt_zscore'] > 3, palette={True: 'red', False: 'blue'})\n",
    "plt.title('Bike Rentals Over Time with Outliers Highlighted (z-score > 3)')\n",
    "plt.xlabel('Record Index')\n",
    "plt.ylabel('Bike Rental Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any temporary columns (like z-score for cnt) if they exist\n",
    "if 'cnt_zscore' in data.columns:\n",
    "    data = data.drop(columns=['cnt_zscore'])\n",
    "\n",
    "# Convert dteday to numeric ordinal and sort the data\n",
    "data['dteday'] = data['dteday'].map(pd.Timestamp.toordinal)\n",
    "data = data.sort_values(by='dteday')\n",
    "\n",
    "# Sequential split into train (60%), validation (20%), and test (20%)\n",
    "n = len(data)\n",
    "train = data.iloc[:int(0.6 * n)].copy()\n",
    "val   = data.iloc[int(0.6 * n):int(0.8 * n)].copy()\n",
    "test  = data.iloc[int(0.8 * n):].copy()\n",
    "\n",
    "print(\"Training set size:\", train.shape[0])\n",
    "print(\"Validation set size:\", val.shape[0])\n",
    "print(\"Test set size:\", test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep full copies for later use\n",
    "X_train_full = train.copy()\n",
    "X_val_full   = val.copy()\n",
    "X_test_full  = test.copy()\n",
    "\n",
    "# Define target variable y\n",
    "y_train = train['cnt']\n",
    "y_val   = val['cnt']\n",
    "y_test  = test['cnt']\n",
    "\n",
    "# Check for missing seasons in training and adjust if necessary\n",
    "train_seasons = set(X_train_full['season'].unique())\n",
    "all_seasons = set(data['season'].unique())\n",
    "missing_seasons = all_seasons - train_seasons\n",
    "if missing_seasons:\n",
    "    print(\"Missing seasons in training data:\", missing_seasons)\n",
    "    rows_to_move = X_val_full[X_val_full['season'].isin(missing_seasons)]\n",
    "    X_train_full = pd.concat([X_train_full, rows_to_move])\n",
    "    y_train = pd.concat([y_train, y_val.loc[rows_to_move.index]])\n",
    "    X_val_full = X_val_full.drop(rows_to_move.index)\n",
    "    y_val = y_val.drop(rows_to_move.index)\n",
    "    print(\"Updated training seasons:\", set(X_train_full['season'].unique()))\n",
    "else:\n",
    "    print(\"All seasons are already present in the training data.\")\n",
    "\n",
    "# Drop unnecessary columns (target and others not needed for modeling)\n",
    "columns_to_drop = ['instant', 'dteday', 'casual', 'registered', 'atemp', 'cnt']\n",
    "X_train = X_train_full.drop(columns=columns_to_drop)\n",
    "X_val   = X_val_full.drop(columns=columns_to_drop)\n",
    "X_test  = X_test_full.drop(columns=columns_to_drop)\n",
    "print(\"Final training feature columns:\", X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups\n",
    "numerical_features = ['temp', 'hum', 'windspeed']\n",
    "categorical_features = ['season', 'mnth', 'weathersit']\n",
    "cyclical_features = ['hr', 'weekday']\n",
    "\n",
    "# Function to encode cyclical features\n",
    "def encode_cyclical_features(df, col, max_val):\n",
    "    df[f'{col}_sin'] = np.sin(2 * np.pi * df[col] / max_val)\n",
    "    df[f'{col}_cos'] = np.cos(2 * np.pi * df[col] / max_val)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies for encoding\n",
    "X_train_encoded = X_train.copy()\n",
    "X_val_encoded   = X_val.copy()\n",
    "X_test_encoded  = X_test.copy()\n",
    "\n",
    "# Encode cyclical features for 'hr' and 'weekday'\n",
    "for col, max_val in [('hr', 24), ('weekday', 7)]:\n",
    "    X_train_encoded = encode_cyclical_features(X_train_encoded, col, max_val)\n",
    "    X_val_encoded   = encode_cyclical_features(X_val_encoded, col, max_val)\n",
    "    X_test_encoded  = encode_cyclical_features(X_test_encoded, col, max_val)\n",
    "\n",
    "# One-hot encode categorical features – reinitialize encoder per column\n",
    "for cat_feature in categorical_features:\n",
    "    encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "    # Fit and transform on training data\n",
    "    encoded_train = encoder.fit_transform(X_train_encoded[[cat_feature]])\n",
    "    feature_names = [f\"{cat_feature}_{i}\" for i in range(encoded_train.shape[1])]\n",
    "    encoded_df_train = pd.DataFrame(encoded_train, columns=feature_names, index=X_train_encoded.index)\n",
    "    X_train_encoded = pd.concat([X_train_encoded, encoded_df_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform validation and test data\n",
    "encoded_val = encoder.transform(X_val_encoded[[cat_feature]])\n",
    "encoded_df_val = pd.DataFrame(encoded_val, columns=feature_names, index=X_val_encoded.index)\n",
    "X_val_encoded = pd.concat([X_val_encoded, encoded_df_val], axis=1)\n",
    "    \n",
    "encoded_test = encoder.transform(X_test_encoded[[cat_feature]])\n",
    "encoded_df_test = pd.DataFrame(encoded_test, columns=feature_names, index=X_test_encoded.index)\n",
    "X_test_encoded = pd.concat([X_test_encoded, encoded_df_test], axis=1)\n",
    "\n",
    "# Felt that my results for models were too good to be true, so I added a leakage chec\n",
    "# ======= LEAKAGE / Consistency Check for One-Hot Encoding =======\n",
    "for cat in categorical_features:\n",
    "    encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "    train_encoded = encoder.fit_transform(X_train[[cat]])\n",
    "    val_encoded = encoder.transform(X_val[[cat]])\n",
    "    feature_names = [f\"{cat}_{i}\" for i in range(train_encoded.shape[1])]\n",
    "    train_df = pd.DataFrame(train_encoded, columns=feature_names, index=X_train.index)\n",
    "    val_df = pd.DataFrame(val_encoded, columns=feature_names, index=X_val.index)\n",
    "    # This assertion ensures that the same dummy columns are created for both sets\n",
    "    assert list(train_df.columns) == list(val_df.columns), f\"Mismatch in encoding for {cat}\"\n",
    "    print(f\"Encoding for '{cat}' is consistent. Columns: {list(train_df.columns)}\")\n",
    "# ======= End Leakage Check =======\n",
    "\n",
    "# Drop original categorical and cyclical columns\n",
    "cols_to_drop = categorical_features + cyclical_features\n",
    "X_train_encoded = X_train_encoded.drop(columns=cols_to_drop)\n",
    "X_val_encoded   = X_val_encoded.drop(columns=cols_to_drop)\n",
    "X_test_encoded  = X_test_encoded.drop(columns=cols_to_drop)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_encoded[numerical_features] = scaler.fit_transform(X_train_encoded[numerical_features])\n",
    "X_val_encoded[numerical_features]   = scaler.transform(X_val_encoded[numerical_features])\n",
    "X_test_encoded[numerical_features]    = scaler.transform(X_test_encoded[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(X_train.index) & set(X_val.index))  # should be empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction term: temp * hum\n",
    "interaction_name = 'temp_hum'\n",
    "\n",
    "# If duplicate exists, rename the new interaction column\n",
    "if interaction_name in X_train_encoded.columns: interaction_name = 'temp_hum_int'\n",
    "X_train_encoded[interaction_name] = X_train_encoded['temp'] * X_train_encoded['hum']\n",
    "X_val_encoded[interaction_name]   = X_val_encoded['temp'] * X_val_encoded['hum']\n",
    "X_test_encoded[interaction_name]  = X_test_encoded['temp'] * X_test_encoded['hum']\n",
    "\n",
    "print(\"Feature engineering completed.\")\n",
    "print(f\"Training features shape: {X_train_encoded.shape}\")\n",
    "print(f\"Feature names: {X_train_encoded.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make validation and test sets match the training set\n",
    "X_val_encoded = X_val_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
    "X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Baseline Model – Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_encoded, y_train)\n",
    "y_train_pred_lr = lr_model.predict(X_train_encoded)\n",
    "y_val_pred_lr = lr_model.predict(X_val_encoded)\n",
    "\n",
    "# Ensure the validation set has the same columns as the training set\n",
    "X_val_encoded = X_val_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
    "y_val_pred_lr = lr_model.predict(X_val_encoded)\n",
    "\n",
    "# Compute residuals from the baseline model predictions\n",
    "residuals_lr = y_val_pred_lr - y_val\n",
    "\n",
    "def evaluate_model(y_true, y_pred, name=\"Model\"):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{name} Performance:\")\n",
    "    print(f\"  MSE: {mse:.2f}\")\n",
    "    print(f\"  RMSE: {rmse:.2f}\")\n",
    "    print(f\"  MAE: {mae:.2f}\")\n",
    "    print(f\"  R² Score: {r2:.4f}\\n\")\n",
    "    return mse, rmse, mae, r2\n",
    "\n",
    "print(\"Linear Regression Training Results:\")\n",
    "lr_train_metrics = evaluate_model(y_train, y_train_pred_lr, \"Linear Regression (Training)\")\n",
    "print(\"Linear Regression Validation Results:\")\n",
    "lr_val_metrics = evaluate_model(y_val, y_val_pred_lr, \"Linear Regression (Validation)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_train_encoded, y_train)\n",
    "y_train_pred_rf = rf_model.predict(X_train_encoded)\n",
    "y_val_pred_rf = rf_model.predict(X_val_encoded)\n",
    "\n",
    "print(\"Random Forest Training Results:\")\n",
    "rf_train_metrics = evaluate_model(y_train, y_train_pred_rf, \"Random Forest (Training)\")\n",
    "print(\"Random Forest Validation Results:\")\n",
    "rf_val_metrics = evaluate_model(y_val, y_val_pred_rf, \"Random Forest (Validation)\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_val_pred_rf, y_val_pred_rf - y_val, alpha=0.5)\n",
    "plt.axhline(0, color='red', linestyle='-')\n",
    "plt.title('RF Residuals vs Predicted')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(y_val_pred_rf - y_val, kde=True)\n",
    "plt.title('RF Residual Distribution')\n",
    "plt.xlabel('Residuals')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "rf_feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_encoded.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=rf_feature_importance.head(15))\n",
    "plt.title('RF Feature Importance (Top 15)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Gradient Boosting Regressor (LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "gb_model.fit(X_train_encoded, y_train)\n",
    "y_train_pred_gb = gb_model.predict(X_train_encoded)\n",
    "y_val_pred_gb = gb_model.predict(X_val_encoded)\n",
    "\n",
    "print(\"Gradient Boosting Training Results:\")\n",
    "gb_train_metrics = evaluate_model(y_train, y_train_pred_gb, \"Gradient Boosting (Training)\")\n",
    "print(\"Gradient Boosting Validation Results:\")\n",
    "gb_val_metrics = evaluate_model(y_val, y_val_pred_gb, \"Gradient Boosting (Validation)\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_val_pred_gb, y_val_pred_gb - y_val, alpha=0.5)\n",
    "plt.axhline(0, color='red', linestyle='-')\n",
    "plt.title('GB Residuals vs Predicted')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(y_val_pred_gb - y_val, kde=True)\n",
    "plt.title('GB Residual Distribution')\n",
    "plt.xlabel('Residuals')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "gb_feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_encoded.columns,\n",
    "    'Importance': gb_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=gb_feature_importance.head(15))\n",
    "plt.title('GB Feature Importance (Top 15)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM model with default parameters\n",
    "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "lgb_model.fit(X_train_encoded, y_train)\n",
    "y_train_pred_lgb = lgb_model.predict(X_train_encoded)\n",
    "y_val_pred_lgb = lgb_model.predict(X_val_encoded)\n",
    "\n",
    "print(\"LightGBM Training Results:\")\n",
    "lgb_train_metrics = evaluate_model(y_train, y_train_pred_lgb, \"LightGBM (Training)\")\n",
    "print(\"LightGBM Validation Results:\")\n",
    "lgb_val_metrics = evaluate_model(y_val, y_val_pred_lgb, \"LightGBM (Validation)\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_val_pred_lgb, y_val_pred_lgb - y_val, alpha=0.5)\n",
    "plt.axhline(0, color='red', linestyle='-')\n",
    "plt.title('LightGBM Residuals vs Predicted')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(y_val_pred_lgb - y_val, kde=True)\n",
    "plt.title('LightGBM Residual Distribution')\n",
    "plt.xlabel('Residuals')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "lgb_feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_encoded.columns,\n",
    "    'Importance': lgb_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=lgb_feature_importance.head(15))\n",
    "plt.title('LightGBM Feature Importance (Top 15)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that lightgbm is better than normal scikit-learns gradientboosting model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Task 7: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning Random Forest Regressor\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator = RandomForestRegressor(random_state=42),\n",
    "    param_distributions = rf_param_grid,\n",
    "    n_iter = 20,\n",
    "    cv = 5,\n",
    "    verbose = 1,\n",
    "    random_state = 42,\n",
    "    n_jobs = -1,\n",
    "    scoring = 'neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "rf_random.fit(X_train_encoded, y_train)\n",
    "print(\"Best Random Forest Parameters:\")\n",
    "print(rf_random.best_params_)\n",
    "\n",
    "best_rf_model = rf_random.best_estimator_\n",
    "y_val_pred_rf_tuned = best_rf_model.predict(X_val_encoded)\n",
    "print(\"Tuned Random Forest Validation Results:\")\n",
    "rf_tuned_val_metrics = evaluate_model(y_val, y_val_pred_rf_tuned, \"Tuned Random Forest (Validation)\")\n",
    "\n",
    "rf_tuned_feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_encoded.columns,\n",
    "    'Importance': best_rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=rf_tuned_feature_importance.head(15))\n",
    "plt.title('Tuned Random Forest Feature Importance (Top 15)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning LightGBM using RandomizedSearchCV\n",
    "lgb_param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'num_leaves': [15, 31, 63],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "lgb_random = RandomizedSearchCV(\n",
    "    estimator=lgb.LGBMRegressor(random_state=42),\n",
    "    param_distributions=lgb_param_grid,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "lgb_random.fit(X_train_encoded, y_train)\n",
    "print(\"Best LightGBM Parameters:\")\n",
    "print(lgb_random.best_params_)\n",
    "\n",
    "# Retrieve the best estimator using the tuned hyperparameters\n",
    "best_lgb_model = lgb_random.best_estimator_\n",
    "y_val_pred_lgb_tuned = best_lgb_model.predict(X_val_encoded)\n",
    "\n",
    "print(\"Tuned LightGBM Validation Results:\")\n",
    "lgb_tuned_val_metrics = evaluate_model(y_val, y_val_pred_lgb_tuned, \"Tuned LightGBM (Validation)\")\n",
    "\n",
    "# Plot feature importance from the tuned LightGBM model\n",
    "lgb_tuned_feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_encoded.columns,\n",
    "    'Importance': best_lgb_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=lgb_tuned_feature_importance.head(15))\n",
    "plt.title('Tuned LightGBM Feature Importance (Top 15)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of Models on Validation Set\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'RF (Default)', 'RF (Tuned)', 'LGBM (Default)', 'LGBM (Tuned)'],\n",
    "    'MSE': [lr_val_metrics[0], rf_val_metrics[0], rf_tuned_val_metrics[0], lgb_val_metrics[0], lgb_tuned_val_metrics[0]],\n",
    "    'RMSE': [lr_val_metrics[1], rf_val_metrics[1], rf_tuned_val_metrics[1], lgb_val_metrics[1], lgb_tuned_val_metrics[1]],\n",
    "    'MAE': [lr_val_metrics[2], rf_val_metrics[2], rf_tuned_val_metrics[2], lgb_val_metrics[2], lgb_tuned_val_metrics[2]],\n",
    "    'R²': [lr_val_metrics[3], rf_val_metrics[3], rf_tuned_val_metrics[3], lgb_val_metrics[3], lgb_tuned_val_metrics[3]]\n",
    "})\n",
    "print(\"Comparison of Models on Validation Set:\")\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8: Iterative Evaluation and Refinement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refining the Linear Regression model by addressing outliers in the training data\n",
    "\n",
    "# Identify outliers in the validation residuals using the z-score method\n",
    "residuals_z = np.abs(zscore(residuals_lr))\n",
    "outlier_val_indices = np.where(residuals_z > 3)[0]\n",
    "print(\"Number of outliers in validation residuals:\", len(outlier_val_indices))\n",
    "\n",
    "# 1. Evaluate training data for potential outliers in the target variable\n",
    "# If a significant number of outliers are detected in training, consider removing them.\n",
    "y_train_z = np.abs(zscore(y_train))\n",
    "outlier_train_indices = np.where(y_train_z > 3)[0]\n",
    "print(\"Number of outliers in training target:\", len(outlier_train_indices))\n",
    "\n",
    "# Here, we create refined training sets by removing identified outliers.\n",
    "X_train_refined = X_train_encoded.copy()\n",
    "y_train_refined = y_train.copy()\n",
    "if len(outlier_train_indices) > 0:\n",
    "    X_train_refined = X_train_refined.drop(index=X_train_refined.index[outlier_train_indices])\n",
    "    y_train_refined = y_train_refined.drop(index=y_train_refined.index[outlier_train_indices])\n",
    "    print(\"Refined training set size:\", X_train_refined.shape[0])\n",
    "else:\n",
    "    print(\"No significant outliers detected in training set.\")\n",
    "\n",
    "# 2. Retrain the Linear Regression model on the refined training set\n",
    "lr_model_refined = LinearRegression()\n",
    "lr_model_refined.fit(X_train_refined, y_train_refined)\n",
    "y_val_pred_lr_refined = lr_model_refined.predict(X_val_encoded)\n",
    "\n",
    "# 3. Evaluate the refined model on the original validation set\n",
    "print(\"Linear Regression (Refined) Validation Results:\")\n",
    "lr_refined_metrics = evaluate_model(y_val, y_val_pred_lr_refined, \"Linear Regression (Refined)\")\n",
    "\n",
    "# 4. Analyze the validation residuals from the initial Linear Regression model\n",
    "residuals_lr = y_val_pred_lr - y_val\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(residuals_lr, kde=True)\n",
    "plt.title(\"Linear Regression Residual Distribution (Before Refinement)\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.show()\n",
    "\n",
    "# 5. Plot the residuals of the refined model for comparison\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(y_val_pred_lr_refined - y_val, kde=True)\n",
    "plt.title(\"Linear Regression Residual Distribution (After Refinement)\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "comparison_lr = pd.DataFrame({\n",
    "    'Metric': [\"MSE\", \"RMSE\", \"MAE\", \"R² Score\"],\n",
    "    'Before Refinement': lr_val_metrics,\n",
    "    'After Refinement': lr_refined_metrics\n",
    "})\n",
    "print(comparison_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't seem to provide better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) Evaluate the original Random Forest model on training data and identify outliers\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_train_encoded, y_train)\n",
    "y_train_pred_rf = rf_model.predict(X_train_encoded)\n",
    "\n",
    "# Identify outliers in the training target using z-score\n",
    "rf_train_z = np.abs(zscore(y_train))\n",
    "outlier_rf_indices = np.where(rf_train_z > 3)[0]\n",
    "print(\"Random Forest: Number of outliers in training:\", len(outlier_rf_indices))\n",
    "\n",
    "# Create refined training set by removing outlier indices\n",
    "X_train_rf_refined = X_train_encoded.copy()\n",
    "y_train_rf_refined = y_train.copy()\n",
    "if len(outlier_rf_indices) > 0:\n",
    "    X_train_rf_refined = X_train_rf_refined.drop(index=X_train_rf_refined.index[outlier_rf_indices])\n",
    "    y_train_rf_refined = y_train_rf_refined.drop(index=y_train_rf_refined.index[outlier_rf_indices])\n",
    "    print(\"Refined training size for Random Forest:\", X_train_rf_refined.shape[0])\n",
    "else:\n",
    "    print(\"No outliers detected for Random Forest refinement.\")\n",
    "\n",
    "# Retrain Random Forest on the refined training set and evaluate on validation\n",
    "rf_model_refined = RandomForestRegressor(random_state=42)\n",
    "rf_model_refined.fit(X_train_rf_refined, y_train_rf_refined)\n",
    "y_val_pred_rf_refined = rf_model_refined.predict(X_val_encoded)\n",
    "print(\"\\nRandom Forest (Refined) Validation Results:\")\n",
    "rf_refined_metrics = evaluate_model(y_val, y_val_pred_rf_refined, \"Random Forest (Refined)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) Evaluate the original LightGBM model on training data and identify outliers\n",
    "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "lgb_model.fit(X_train_encoded, y_train)\n",
    "y_train_pred_lgb = lgb_model.predict(X_train_encoded)\n",
    "\n",
    "# Identify outliers in the training target using z-score\n",
    "lgb_train_z = np.abs(zscore(y_train))\n",
    "outlier_lgb_indices = np.where(lgb_train_z > 3)[0]\n",
    "print(\"LightGBM: Number of outliers in training:\", len(outlier_lgb_indices))\n",
    "\n",
    "# Create refined training set by removing outlier indices\n",
    "X_train_lgb_refined = X_train_encoded.copy()\n",
    "y_train_lgb_refined = y_train.copy()\n",
    "if len(outlier_lgb_indices) > 0:\n",
    "    X_train_lgb_refined = X_train_lgb_refined.drop(index=X_train_lgb_refined.index[outlier_lgb_indices])\n",
    "    y_train_lgb_refined = y_train_lgb_refined.drop(index=y_train_lgb_refined.index[outlier_lgb_indices])\n",
    "    print(\"Refined training size for LightGBM:\", X_train_lgb_refined.shape[0])\n",
    "else:\n",
    "    print(\"No outliers detected for LightGBM refinement.\")\n",
    "\n",
    "# Retrain LightGBM on the refined training set and evaluate on validation\n",
    "lgb_model_refined = lgb.LGBMRegressor(random_state=42)\n",
    "lgb_model_refined.fit(X_train_lgb_refined, y_train_lgb_refined)\n",
    "y_val_pred_lgb_refined = lgb_model_refined.predict(X_val_encoded)\n",
    "print(\"\\nLightGBM (Refined) Validation Results:\")\n",
    "lgb_refined_metrics = evaluate_model(y_val, y_val_pred_lgb_refined, \"LightGBM (Refined)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, we use dummy lists; update them accordingly.\n",
    "rf_metrics_before = [rf_train_metrics[0], rf_train_metrics[1], rf_train_metrics[2], rf_train_metrics[3]]  # e.g., [MSE, RMSE, MAE, R²]\n",
    "rf_metrics_after  = [rf_refined_metrics[0], rf_refined_metrics[1], rf_refined_metrics[2], rf_refined_metrics[3]]\n",
    "\n",
    "lgb_metrics_before = [lgb_train_metrics[0], lgb_train_metrics[1], lgb_train_metrics[2], lgb_train_metrics[3]]\n",
    "lgb_metrics_after  = [lgb_refined_metrics[0], lgb_refined_metrics[1], lgb_refined_metrics[2], lgb_refined_metrics[3]]\n",
    "\n",
    "comparison_rf = pd.DataFrame({\n",
    "    \"Metric\": [\"MSE\", \"RMSE\", \"MAE\", \"R² Score\"],\n",
    "    \"Random Forest Before\": rf_metrics_before,\n",
    "    \"Random Forest After\": rf_metrics_after\n",
    "})\n",
    "comparison_lgb = pd.DataFrame({\n",
    "    \"Metric\": [\"MSE\", \"RMSE\", \"MAE\", \"R² Score\"],\n",
    "    \"LightGBM Before\": lgb_metrics_before,\n",
    "    \"LightGBM After\": lgb_metrics_after\n",
    "})\n",
    "\n",
    "print(\"\\nRandom Forest Comparison (Before vs. After Refinement):\")\n",
    "print(comparison_rf)\n",
    "print(\"\\nLightGBM Comparison (Before vs. After Refinement):\")\n",
    "print(comparison_lgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like over fitting for the training data? I did a check to ensure no leakage in step 3 but still getting the exact same values before. Very large difference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 9: Final Model Selection and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute validation metrics for each model if not already defined\n",
    "lr_metrics_original = evaluate_model(y_val, y_val_pred_lr, \"Linear Regression (Original)\")\n",
    "rf_metrics_original = evaluate_model(y_val, y_val_pred_rf, \"Random Forest (Original)\")\n",
    "lgb_metrics_original = evaluate_model(y_val, y_val_pred_lgb, \"LightGBM (Original)\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REALLY LOW error rates for random forest & LightGBM model, asked gpt but it said it could be due to over fitting instead. Still trying to find if any/where data leakage could have occured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training and validation sets\n",
    "X_train_val = pd.concat([X_train_encoded, X_val_encoded], axis=0)\n",
    "y_train_val = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "# Retrain best model (LightGBM) on full train+val set\n",
    "final_lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "final_lgb_model.fit(X_train_val, y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which columns are missing in validation\n",
    "missing_in_val = set(X_train_encoded.columns) - set(X_val_encoded.columns)\n",
    "extra_in_val = set(X_val_encoded.columns) - set(X_train_encoded.columns)\n",
    "\n",
    "print(\" Missing in validation (present in training):\", missing_in_val)\n",
    "print(\" Extra in validation (not in training):\", extra_in_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training + validation\n",
    "X_train_val = pd.concat([X_train_encoded, X_val_encoded])\n",
    "y_train_val = pd.concat([y_train, y_val])\n",
    "\n",
    "# Align test set again (in case train+val added new columns)\n",
    "X_test_encoded = X_test_encoded.reindex(columns=X_train_val.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "y_test_pred = final_lgb_model.predict(X_test_encoded)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, y_test_pred)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "final_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\" Final Model Evaluation on Test Set:\")\n",
    "print(f\"  MSE: {final_mse:.2f}\")\n",
    "print(f\"  RMSE: {final_rmse:.2f}\")\n",
    "print(f\"  MAE: {final_mae:.2f}\")\n",
    "print(f\"  R² Score: {final_r2:.4f}\")\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Residuals vs Predicted\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test_pred, y_test_pred - y_test, alpha=0.5)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title('Residuals vs Predicted (Test Set)')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "\n",
    "# Distribution of Residuals\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(y_test_pred - y_test, kde=True)\n",
    "plt.title('Residual Distribution (Test Set)')\n",
    "plt.xlabel('Residuals')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model generalises well compared to our baseline models. Tunning by RandomizedSearchCV has improved generalization by optimizing critical hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Never trained a model like this, but it feels like the model is doing really well. Is this possible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
